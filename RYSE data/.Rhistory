geom_point() +
facet_wrap(~subject, ncol = 5) +
labs(x = "Days of sleep deprivation",
y = "Average reaction time (ms)") +
scale_x_continuous(breaks = 0:4 * 2) +
theme(strip.text = element_text(size = 12),
axis.text.y = element_text(size = 12))
fit.random_slope = lmer(formula = reaction ~ 1 + days + (0 + days | subject),
data = df.sleep)
fit.random_slope %>%
augment() %>%
clean_names() %>%
ggplot(data = .,
mapping = aes(x = days,
y = reaction)) +
geom_line(aes(y = fitted),
color = "blue") +
geom_point() +
facet_wrap(vars(subject), ncol = 5) +
labs(x = "Days of sleep deprivation",
y = "Average reaction time (ms)") +
scale_x_continuous(breaks = 0:4 * 2) +
theme(strip.text = element_text(size = 12),
axis.text.y = element_text(size = 12))
ggplot(data = df.pooling,
mapping = aes(x = days,
y = reaction)) +
geom_smooth(method = "lm",
se = F,
color = "orange",
fullrange = T) +
geom_line(aes(y = complete_pooling),
color = "green") +
geom_line(aes(y = partial_pooling),
color = "blue") +
geom_point() +
facet_wrap(~subject, ncol = 5) +
labs(x = "Days of sleep deprivation",
y = "Average reaction time (ms)") +
scale_x_continuous(breaks = 0:4 * 2) +
theme(strip.text = element_text(size = 12),
axis.text.y = element_text(size = 12))
# complete pooling
fit.complete_pooling = lm(formula = reaction ~ days,
data = df.sleep)
df.complete_pooling =  fit.complete_pooling %>%
augment() %>%
bind_rows(fit.complete_pooling %>%
augment(newdata = tibble(subject = c("373", "374"),
days = rep(10, 2)))) %>%
clean_names() %>%
select(reaction, days, complete_pooling = fitted)
# no pooling
df.no_pooling = df.sleep %>%
group_by(subject) %>%
nest(data = c(days, reaction)) %>%
mutate(fit = map(.x = data,
.f = ~ lm(reaction ~ days, data = .x)),
augment = map(.x = fit,
.f = ~ augment(.x))) %>%
unnest(c(augment)) %>%
ungroup() %>%
clean_names() %>%
select(subject, reaction, days, no_pooling = fitted)
# partial pooling
fit.lmer = lmer(formula = reaction ~ 1 + days + (1 + days | subject),
data = df.sleep)
df.partial_pooling = fit.lmer %>%
augment() %>%
bind_rows(fit.lmer %>%
augment(newdata = tibble(subject = c("373", "374"),
days = rep(10, 2)))) %>%
clean_names() %>%
select(subject, reaction, days, partial_pooling = fitted)
# combine results
df.pooling = df.partial_pooling %>%
left_join(df.complete_pooling,
by = c("reaction", "days")) %>%
left_join(df.no_pooling,
by = c("subject", "reaction", "days"))
ggplot(data = df.pooling,
mapping = aes(x = days,
y = reaction)) +
geom_smooth(method = "lm",
se = F,
color = "orange",
fullrange = T) +
geom_line(aes(y = complete_pooling),
color = "green") +
geom_line(aes(y = partial_pooling),
color = "blue") +
geom_point() +
facet_wrap(~subject, ncol = 5) +
labs(x = "Days of sleep deprivation",
y = "Average reaction time (ms)") +
scale_x_continuous(breaks = 0:4 * 2) +
theme(strip.text = element_text(size = 12),
axis.text.y = element_text(size = 12))
# subselection
ggplot(data = df.pooling %>%
filter(subject %in% c("373", "374")),
mapping = aes(x = days,
y = reaction)) +
geom_smooth(method = "lm",
se = F,
color = "orange",
fullrange = T) +
geom_line(aes(y = complete_pooling),
color = "green") +
geom_line(aes(y = partial_pooling),
color = "blue") +
geom_point() +
facet_wrap(vars(subject)) +
labs(x = "Days of sleep deprivation",
y = "Average reaction time (ms)") +
scale_x_continuous(breaks = 0:4 * 2) +
theme(strip.text = element_text(size = 12),
axis.text.y = element_text(size = 12))
# get estimates from partial pooling model
df.partial_pooling = fit.random_intercept_slope %>%
coef() %>%
.$subject %>%
rownames_to_column("subject") %>%
clean_names()
# combine estimates from no pooling with partial pooling model
df.plot = df.sleep %>%
group_by(subject) %>%
nest(data = c(days, reaction)) %>%
mutate(fit = map(.x = data,
.f = ~ lm(reaction ~ days, data = .x)),
tidy = map(.x = fit,
.f = ~ tidy(.x))) %>%
unnest(c(tidy)) %>%
select(subject, term, estimate) %>%
pivot_wider(names_from = term,
values_from = estimate) %>%
clean_names() %>%
mutate(method = "no pooling") %>%
bind_rows(df.partial_pooling %>%
mutate(method = "partial pooling")) %>%
pivot_longer(cols = -c(subject, method),
names_to = "index",
values_to = "value") %>%
mutate(index = factor(index, levels = c("intercept", "days")))
# visualize the results
ggplot(data = df.plot,
mapping = aes(x = value,
group = method,
fill = method)) +
stat_density(position = "identity",
geom = "area",
color = "black",
alpha = 0.3) +
facet_grid(cols = vars(index),
scales = "free")
lmer(formula = reaction ~ 1 + days + (1 + days | subject), data = df.sleep) %>% joint_tests()
?joint_tests
fit1 = lmer(formula = reaction ~ 1 + days + (1 + days | subject),
data = df.sleep)
fit2 = lmer(formula = reaction ~ 1 + (1 + days | subject),
data = df.sleep)
anova(fit1, fit2)
# using the plot() function
ggpredict(model = fit.random_intercept_slope,
terms = "days",
type = "fixed") %>%
plot()
# using our own ggplot magic
df.plot = ggpredict(model = fit.random_intercept_slope,
terms = "days",
type = "fixed")
ggplot(data = df.plot,
mapping = aes(x = x,
y = predicted,
ymin = conf.low,
ymax = conf.high)) +
geom_ribbon(fill = "lightblue") +
geom_line(linewidth = 1)
lmer(formula = reaction ~ 1 + days + (1 + days | subject),
data = df.sleep) %>%
check_model()
?ggpredict
# make example reproducible
set.seed(1)
# parameters
sample_size = 100
b0 = 1
b1 = 2
sd_residual = 1
sd_participant = 0.5
# generate the data
df.mixed = tibble(participant = rep(1:sample_size, 2),
condition = rep(0:1, each = sample_size)) %>%
group_by(participant) %>%
mutate(intercepts = rnorm(n = 1, sd = sd_participant)) %>%
ungroup() %>%
mutate(value = b0 + b1 * condition + intercepts + rnorm(n(), sd = sd_residual)) %>%
arrange(participant, condition)
df.mixed
# fit model
fit.mixed = lmer(formula = value ~ 1 + condition + (1 | participant),
data = df.mixed)
summary(fit.mixed)
fit.mixed %>%
augment() %>%
clean_names() %>%
ggplot(data = .,
mapping = aes(x = condition,
y = value,
group = participant)) +
geom_point(alpha = 0.5) +
geom_line(alpha = 0.5) +
geom_point(aes(y = fitted),
color = "red") +
geom_line(aes(y = fitted),
color = "red")
# using chisq test
fit.compact = lm(formula = value ~ 1 +  condition,
data = df.mixed)
fit.augmented = lmer(formula = value ~ 1 + condition +  (1 | participant),
data = df.mixed)
anova(fit.augmented, fit.compact)
# let's make one outlier
df.outlier = df.mixed %>%
mutate(participant = participant %>% as.character() %>% as.numeric()) %>%
filter(participant <= 20) %>%
mutate(value = ifelse(participant == 20, value + 30, value),
participant = as.factor(participant))
# fit model
fit.outlier = lmer(formula = value ~ 1 + condition + (1 | participant),
data = df.outlier)
summary(fit.outlier)
fit.outlier %>%
augment() %>%
clean_names() %>%
ggplot(data = .,
mapping = aes(x = condition,
y = value,
group = participant)) +
geom_point(alpha = 0.5) +
geom_line(alpha = 0.5) +
geom_point(aes(y = fitted),
color = "red") +
geom_line(aes(y = fitted),
color = "red")
# simulated data from lmer with outlier
fit.outlier %>%
simulate() %>%
bind_cols(df.outlier) %>%
ggplot(data = .,
mapping = aes(x = condition,
y = sim_1,
group = participant)) +
geom_line(alpha = 0.5) +
geom_point(alpha = 0.5)
# make example reproducible
set.seed(1)
tmp = rnorm(n = 20)
df.slopes = tibble(
condition = rep(1:2, each = 20),
participant = rep(1:20, 2),
value = ifelse(condition == 1, tmp,
mean(tmp) + rnorm(n = 20, sd = 0.3)) # regression to the mean
) %>%
mutate(condition = as.factor(condition),
participant = as.factor(participant))
# make example reproducible
set.seed(1)
tmp = rnorm(n = 20)
df.slopes = tibble(
condition = rep(1:2, each = 20),
participant = rep(1:20, 2),
value = ifelse(condition == 1, tmp,
mean(tmp) + rnorm(n = 20, sd = 0.3)) # regression to the mean
) %>%
mutate(condition = as.factor(condition),
participant = as.factor(participant))
# model fit
fit.slopes = lmer(formula = value ~ 1 + condition + (1 | participant),
data = df.slopes)
# make example reproducible
set.seed(1)
tmp = rnorm(n = 20)
df.slopes = tibble(
condition = rep(1:2, each = 20),
participant = rep(1:20, 2),
value = ifelse(condition == 1, tmp,
mean(tmp) + rnorm(n = 20, sd = 0.3)) # regression to the mean
) %>%
mutate(condition = as.factor(condition),
participant = as.factor(participant))
# model fit
fit.slopes = lmer(formula = value ~ 1 + condition + (1 | participant),
data = df.slopes)
summary(fit.slopes)
# fit model
lmer(formula = value ~ 1 + condition + (1 + condition | participant),
data = df.slopes)
?plot
library(ggplot2)
?geom_line
# Cross-sectional data analysis of the data from the RYSE study.
## Packages ######
library(car) # lm testing
library(caret) # confusion matrix
library(corrr) # correlation
library(corrplot) # plot
library(dplyr) # dataframe managment
library(e1071) #NaiveBayes
library(factoextra) # PCA
library(FactoMineR) # PCA
library(Factoshiny) # PCA
library(ggplot2) # ploting
library(generics)
library(gridExtra) # plotinh
library(haven) # read sav data
library(lmtest) # lm testing
library(MASS) # for stepAIC + lda + qda
library(missForest) # to impute data
library(olsrr) # influence statistic
library(poLCA) # Latent Class Analysis
library(rpart) # Classification trees
library(rstanarm) # bayesian lm
library(sandwich) # lm testing
library(tidyLPA) # LPA
library(tidymodels) # multiclass classification regression
## Import work ####
setwd("~/Ecole/M1/Stage/Internship_repo/RYSE data")
load("~/Ecole/M1/Stage/Internship_repo/RYSE data/.RData")
View(all_groups_BDI_Engagement)
View(df_n_groups_BDI_Engagement)
View(df_result_BDI_Engagement)
# PCA/EFA on Perception of Neighborhood
res.PCA<-PCA(df_SAr[, c(paste0("T1_PoNS_", 1:8))],graph=FALSE)
res.PCA$eig
fviz_screeplot(X=res.PCA, addlabels = TRUE, ylim = c(0, 50))
fviz_pca_var(res.PCA, axes = 1 :2,col.var = "cos2",gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE)
fviz_pca_var(res.PCA, axes = 3:4,col.var = "cos2",gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE)
corrplot(cor(df_SAr[, c(paste0("T1_PoNS_", 1:8))]))
#Comments : We can see two groups emerge for the variable :
# 1,2,4,5,7 which are positively correlated together and are characterized by relatively big value on dim1 and small positive value on dim2. We don't see the group again on dim 3 and 4.
# 3, 6 and 8 which are positively correlated together and are characterized by a small negative value on dim1 and a relatively big value on dim2. Again we don't see this grouping again.
# Good to note that questions 3,6 and 8 are the questions that need to be "reversed".
# The two groups are at a 90Â° angle which would mean a small correlation between them.
grid.arrange(plot_3_5,plot_1_3,plot_2_3, ncol = 3)
View(df_SAr)
View(df_perf_classification_tree)
df_perf_classification_tree <- estimation_classification_tree_with_test(df,df_result_BDI_Engagement,"Engagement",groups_to_test,n_perm=1000,predictors = explication_vars)
df_long <- df_perf_regression_tree %>%
pivot_longer(cols = c(R.squared, R.squared.adjusted),
names_to = "metric",
values_to = "value")
plot_performance <- ggplot(df_long, aes(x = average_group_size, y = value, color = metric)) +
geom_line(size = 1) +
geom_point() +
labs(title = "Performance Metrics vs. Group Size",
x = "Average Group Size",
y = "Performance",
color = "Metric") +
theme_minimal()
df_long <- df_perf_regression_tree %>%
pivot_longer(cols = c(MAE,RMSE),
names_to = "metric",
values_to = "value")
plot_error <- ggplot(df_long, aes(x = average_group_size, y = value, color = metric)) +
geom_line(size = 1) +
geom_point() +
labs(title = "Performance Metrics vs. Group Size",
x = "Average Group Size",
y = "Error",
color = "Metric") +
theme_minimal()
grid.arrange(plot_performance, plot_error, ncol = 2)
# Regression tree to predict transformation residuals
df_perf_regression_tree_multiplication <- regression_tree(df,df_result_BDI_Engagement,groups_to_test,predictors = explication_vars,method = "multiply")
# Visualization of the result
df_long <- df_perf_regression_tree_multiplication %>%
pivot_longer(cols = c(R.squared, R.squared.adjusted),
names_to = "metric",
values_to = "value")
plot_performance_multiply <- ggplot(df_long, aes(x = average_group_size, y = value, color = metric)) +
geom_line(size = 1) +
geom_point() +
labs(title = "Performance Metrics vs. Group Size",
x = "Average Group Size",
y = "Performance",
color = "Metric") +
theme_minimal()
df_long <- df_perf_regression_tree_multiplication %>%
pivot_longer(cols = c(MAE,RMSE),
names_to = "metric",
values_to = "value")
plot_error_multiply <- ggplot(df_long, aes(x = average_group_size, y = value, color = metric)) +
geom_line(size = 1) +
geom_point() +
labs(title = "Performance Metrics vs. Group Size",
x = "Average Group Size",
y = "Error",
color = "Metric") +
theme_minimal()
grid.arrange(plot_performance_multiply, plot_error_multiply, ncol = 2)
df_perf_regression_tree_multiplication <- regression_tree(df,df_result_BDI_Engagement,groups_to_test,predictors = explication_vars,method = "log_multiply")
# Visualization of the result
df_long <- df_perf_regression_tree_multiplication %>%
pivot_longer(cols = c(R.squared, R.squared.adjusted),
names_to = "metric",
values_to = "value")
plot_performance_multiply <- ggplot(df_long, aes(x = average_group_size, y = value, color = metric)) +
geom_line(size = 1) +
geom_point() +
labs(title = "Performance Metrics vs. Group Size",
x = "Average Group Size",
y = "Performance",
color = "Metric") +
theme_minimal()
df_long <- df_perf_regression_tree_multiplication %>%
pivot_longer(cols = c(MAE,RMSE),
names_to = "metric",
values_to = "value")
plot_error_multiply <- ggplot(df_long, aes(x = average_group_size, y = value, color = metric)) +
geom_line(size = 1) +
geom_point() +
labs(title = "Performance Metrics vs. Group Size",
x = "Average Group Size",
y = "Error",
color = "Metric") +
theme_minimal()
grid.arrange(plot_performance_multiply, plot_error_multiply, ncol = 2)
df_perf_regression_tree_multiplication <- regression_tree(df,df_result_BDI_Engagement,groups_to_test,predictors = explication_vars,method = "log_multiply_divide")
# Visualization of the result
df_long <- df_perf_regression_tree_multiplication %>%
pivot_longer(cols = c(R.squared, R.squared.adjusted),
names_to = "metric",
values_to = "value")
plot_performance_multiply <- ggplot(df_long, aes(x = average_group_size, y = value, color = metric)) +
geom_line(size = 1) +
geom_point() +
labs(title = "Performance Metrics vs. Group Size",
x = "Average Group Size",
y = "Performance",
color = "Metric") +
theme_minimal()
df_long <- df_perf_regression_tree_multiplication %>%
pivot_longer(cols = c(MAE,RMSE),
names_to = "metric",
values_to = "value")
plot_error_multiply <- ggplot(df_long, aes(x = average_group_size, y = value, color = metric)) +
geom_line(size = 1) +
geom_point() +
labs(title = "Performance Metrics vs. Group Size",
x = "Average Group Size",
y = "Error",
color = "Metric") +
theme_minimal()
grid.arrange(plot_performance_multiply, plot_error_multiply, ncol = 2)
df_perf_regression_tree_multiplication <- regression_tree(df,df_result_BDI_Engagement,groups_to_test,predictors = explication_vars,method = "multiply_divide")
# Visualization of the result
df_long <- df_perf_regression_tree_multiplication %>%
pivot_longer(cols = c(R.squared, R.squared.adjusted),
names_to = "metric",
values_to = "value")
plot_performance_multiply <- ggplot(df_long, aes(x = average_group_size, y = value, color = metric)) +
geom_line(size = 1) +
geom_point() +
labs(title = "Performance Metrics vs. Group Size",
x = "Average Group Size",
y = "Performance",
color = "Metric") +
theme_minimal()
df_long <- df_perf_regression_tree_multiplication %>%
pivot_longer(cols = c(MAE,RMSE),
names_to = "metric",
values_to = "value")
plot_error_multiply <- ggplot(df_long, aes(x = average_group_size, y = value, color = metric)) +
geom_line(size = 1) +
geom_point() +
labs(title = "Performance Metrics vs. Group Size",
x = "Average Group Size",
y = "Error",
color = "Metric") +
theme_minimal()
grid.arrange(plot_performance_multiply, plot_error_multiply, ncol = 2)
df_perf_regression_tree_multiplication <- regression_tree(df,df_result_BDI_Engagement,groups_to_test,predictors = explication_vars,method = "log_multiply")
# Visualization of the result
df_long <- df_perf_regression_tree_multiplication %>%
pivot_longer(cols = c(R.squared, R.squared.adjusted),
names_to = "metric",
values_to = "value")
plot_performance_multiply <- ggplot(df_long, aes(x = average_group_size, y = value, color = metric)) +
geom_line(size = 1) +
geom_point() +
labs(title = "Performance Metrics vs. Group Size",
x = "Average Group Size",
y = "Performance",
color = "Metric") +
theme_minimal()
df_long <- df_perf_regression_tree_multiplication %>%
pivot_longer(cols = c(MAE,RMSE),
names_to = "metric",
values_to = "value")
plot_error_multiply <- ggplot(df_long, aes(x = average_group_size, y = value, color = metric)) +
geom_line(size = 1) +
geom_point() +
labs(title = "Performance Metrics vs. Group Size",
x = "Average Group Size",
y = "Error",
color = "Metric") +
theme_minimal()
grid.arrange(plot_performance_multiply, plot_error_multiply, ncol = 2)
df_perf_classification_tree <- estimation_classification_tree_with_test(df,df_result_BDI_Engagement,"Engagement",groups_to_test,n_perm=500,predictors = explication_vars)
grid.arrange(plot_3_5,plot_1_3,plot_2_3, ncol = 3)
View(df_perf_regression_tree)
