groups[idx] <- "vulnerable"
} else if (cluster_label == min_group) {
groups[idx] <- "resilient"
} else {
groups[idx] <- "average"
}
}
}
return(groups)
}
visualization_groups <- function(df,adversity,outcome,adjusted_lm,groups,main="Clusturing results"){
# Adjusted linear regression coefficient for the plot
intercept <- coef(adjusted_lm)[1]
slope     <- coef(adjusted_lm)[2]
# Add groups to the temporary df to color the points
df$group <- factor(groups, levels = c("resilient", "average", "vulnerable",NA))
# Get the limit values of the residuals to visualize the separation of the groups
df$residuals <- df[[outcome]] - (intercept + slope*df[[adversity]])
# Resilient = positive residuals, vulnerable = negative residuals
if(slope<0){
resilient_limit <- min(df[df$group=="resilient",]$residuals, na.rm = TRUE)
vulnerable_limit <- max(df[df$group=="vulnerable",]$residuals, na.rm = TRUE)
}
# Resilient = negative residuals, vulnerable = positive residuals
else{
resilient_limit <- max(df[df$group=="resilient",]$residuals, na.rm = TRUE)
vulnerable_limit <- min(df[df$group=="vulnerable",]$residuals, na.rm = TRUE)
}
# Viz
plot <- ggplot(df, aes(x = .data[[adversity]], y = .data[[outcome]], color = group)) +
geom_point(shape=1,size=0.8) +
geom_abline(intercept = intercept, slope = slope, color = "grey", linetype = "solid") +
labs(
x = adversity,
y = outcome,
title = main,
color = "Group"
) +
theme_minimal(base_size = 10) +
theme(plot.title = element_text(size = 10))+
scale_color_manual(values = c("resilient" = "skyblue", "average" = "grey", "vulnerable" = "coral"))
# Add resilient and vulnerable text + lines for the seperation of the groups
if(slope < 0){
plot <-plot + geom_text(x=max(na.omit(df[[adversity]]))-5,y=max(na.omit(df[[outcome]]))-5,label="Resilient",alpha=0.2,color="grey") + geom_text(x=min(na.omit(df[[adversity]]))+5,y=min(na.omit(df[[outcome]]))+5,label="Vulnerable",alpha=0.2,color="grey")
plot <- plot +
geom_abline(intercept = intercept+resilient_limit, slope = slope, color = "grey", linetype = "dashed")+
geom_abline(intercept = intercept+vulnerable_limit, slope = slope, color = "grey", linetype = "dashed")
}
else{
plot <- plot + geom_text(x=min(na.omit(df[[adversity]]))+5,y=max(na.omit(df[[outcome]]))-5,label="Vulnerable",alpha=0.2,color="grey") + geom_text(x=max(na.omit(df[[adversity]]))-5,y=min(na.omit(df[[outcome]]))+5,label="Resilient",alpha=0.2,color="grey")
plot <- plot +
geom_abline(intercept = intercept+resilient_limit, slope = slope, color = "grey", linetype = "dashed")+
geom_abline(intercept = intercept+vulnerable_limit, slope = slope, color = "grey", linetype = "dashed")
}
return(plot)
}
## Functions : Get all groups + transformation of residuals ####
transform_residuals<-function(residuals,adversity,is_resilience_positive,method){
res <- c()
for(i in 1:length(residuals)){
if(method=="multiply"){
res[i] <- residuals[i]*adversity[i]
}
else if(method=="log_multiply"){
res[i] <- residuals[i]*log(1+adversity[i])
}
else if(method=="multiply_divide"){
if(is_resilience_positive){
res_i <- residuals[i]
if(res_i>=0){
res[i] <- res_i*adversity[i]
}
else{
res[i] <- res_i/adversity[i]
}
}
else{
res_i <- residuals[i]
if(res_i>=0){
res[i] <- res_i/adversity[i]
}
else{
res[i] <- res_i*adversity[i]
}
}
}
else if(method=="log_multiply_divide"){
if(is_resilience_positive){
res_i <- residuals[i]
if(res_i>=0){
res[i] <- res_i*log(1+adversity[i])
}
else{
res[i] <- res_i/log(1+adversity[i])
}
}
else{
res_i <- residuals[i]
if(res_i>=0){
res[i] <- res_i/log(1+adversity[i])
}
else{
res[i] <- res_i*log(1+adversity[i])
}
}
}
}
print(head(res,10))
return(res)
}
# Function to get a dataframe with all of the grouping methods result and the dataframe with the sizes of each group for each method
get_all_groups <- function(df,adversity_string,outcome_string,bins,res,modification="nothing",visualization=TRUE){
outcome <- df[[outcome_string]]
adversity <- df[[adversity_string]]
# Initialize the result data_frames : one with the grouping for each person and each method and one with the number of people in each group for each method
df_n_groups <- data.frame(resilient=c(),average=c(),vulnerable=c())
df_result <- data.frame(residuals=res$residuals_adjusted,adversity=adversity)
# Get the info
lm_adjusted <- res$lm_adjusted
lm_adjusted_cred <- res$lm_adjusted_cred
plot <- res$plot
resilience_sign <- lm_adjusted$coefficients[2]<0
if(modification!="nothing"){
residuals <- transform_residuals(res$residuals_adjusted,adversity,resilience_sign,method=modification)
}
else{
residuals <- res$residuals_adjusted
}
# Raw residuals
groups_raw <- get_groups_raw_residuals(residuals,is_resilience_positive=resilience_sign)
df_n_groups <- rbind(df_n_groups,data.frame(resilient = sum(groups_raw=="resilient", na.rm=TRUE), average = sum(groups_raw=="average", na.rm=TRUE), vulnerable = sum(groups_raw=="vulnerable", na.rm=TRUE), row.names=c("raw")))
df_result[["raw"]] <- groups_raw
if(visualization){
print(visualization_raw_residuals(df,adversity_string,outcome_string,lm_adjusted,groups_raw))
}
# Prediction and confidence intervals
preds_conf <- list(
as.data.frame(predict(lm_adjusted, newdata = df, interval = "prediction", level = 0.75)),
as.data.frame(predict(lm_adjusted, newdata = df, interval = "prediction", level = 0.6)),
as.data.frame(predict(lm_adjusted, newdata = df, interval = "prediction", level = 0.5)),
as.data.frame(predict(lm_adjusted, newdata = df, interval = "confidence", level = 0.99)),
as.data.frame(predict(lm_adjusted, newdata = df, interval = "confidence", level = 0.95))
)
names_conf <- list(
"pred. residuals (75%)",
"pred. residuals (60%)",
"pred. residuals (50%)",
"conf. residuals (99%)",
"conf. residuals (95%)"
)
for(i in 1:length(preds_conf)){
groups <- get_groups_intervals(outcome, preds_conf[[i]],is_resilience_positive=resilience_sign)
df_n_groups <- rbind(df_n_groups,
data.frame(resilient = sum(groups=="resilient", na.rm=TRUE), average = sum(groups=="average", na.rm=TRUE), vulnerable = sum(groups=="vulnerable", na.rm=TRUE), row.names=c(names_conf[[i]])))
df_result[[names_conf[[i]]]] <- groups
}
if(visualization){
print(visualization_intervals(df=df,adversity=adversity_string,outcome=outcome_string,adjusted_lm =lm_adjusted,preds_conf,names_conf,main="Confidence and prediction intervals"))
}
# Quantiles
list_quantile_sub <- list(0.05,0.1,0.15,0.2,0.25)
list_quantile_sup <- list(0.05,0.1,0.15,0.2,0.25)
names_quant <- list(
"quantiles (5%)",
"quantiles (10%)",
"quantiles (15%)",
"quantiles (20%)",
"quantiles (25%)"
)
for(i in 1:length(list_quantile_sub)){
groups <- get_groups_quantile(residuals,list_quantile_sub[[i]],list_quantile_sup[[i]],is_resilience_positive=resilience_sign)
df_n_groups <- rbind(df_n_groups,
data.frame(resilient = sum(groups=="resilient", na.rm=TRUE), average = sum(groups=="average", na.rm=TRUE), vulnerable = sum(groups=="vulnerable", na.rm=TRUE), row.names=c(names_quant[[i]])))
df_result[[names_quant[[i]]]] <- groups
}
# Credibility intervals
preds_cred <- list(
get_credibility_intervals(lm_adjusted_cred,newdata=df[c(adversity_string)],lwr=0.0005,upr=0.9995),
get_credibility_intervals(lm_adjusted_cred,newdata=df[c(adversity_string)],lwr=0.005,upr=0.995),
get_credibility_intervals(lm_adjusted_cred,newdata=df[c(adversity_string)],lwr=0.025,upr=0.975),
get_credibility_intervals(lm_adjusted_cred,newdata=df[c(adversity_string)],lwr=0.05,upr=0.95),
get_credibility_intervals(lm_adjusted_cred,newdata=df[c(adversity_string)],lwr=0.125,upr=0.875),
get_credibility_intervals(lm_adjusted_cred,newdata=df[c(adversity_string)],lwr=0.25,upr=0.75)
)
names_cred <- list(
"cred. 99.9%",
"cred. 99%",
"cred. 95%",
"cred. 90%",
"cred. 75%",
"cred. 50%"
)
for(i in 1:length(preds_cred)){
groups <- get_groups_intervals(outcome, preds_cred[[i]],is_resilience_positive=resilience_sign)
df_n_groups <- rbind(df_n_groups,
data.frame(resilient = sum(groups=="resilient", na.rm=TRUE), average = sum(groups=="average", na.rm=TRUE), vulnerable = sum(groups=="vulnerable", na.rm=TRUE), row.names=c(names_cred[[i]])))
df_result[[names_cred[[i]]]] <- groups
}
if(visualization){
print(visualization_intervals(df=df,adversity=adversity_string,outcome=outcome_string,adjusted_lm =lm_adjusted_cred,preds_cred,names_cred,main="Credibility intervals"))
}
# Standard deviation
list_sd_multiplicator <- list(2,1,0.5)
names_sd <- list("2SD","1SD","0.5SD")
res_sd <- list()
for(i in 1:length(list_sd_multiplicator)){
res_sd[[i]] <- get_groups_sd(df, residuals, bins, adversity_string, resilience_sign, sd_multiplicator=list_sd_multiplicator[[i]])
groups <- res_sd[[i]]$groups_sd
df_n_groups <- rbind(df_n_groups,
data.frame(resilient = sum(groups=="resilient", na.rm=TRUE), average = sum(groups=="average", na.rm=TRUE), vulnerable = sum(groups=="vulnerable", na.rm=TRUE), row.names=c(names_sd[[i]])))
df_result[[names_sd[[i]]]] <- groups
}
if(visualization){
print(visualization_sd_intervals(df,adversity=adversity_string,outcome=outcome_string,adjusted_lm=lm_adjusted,bins=bins,res=res_sd,names_sd=names_sd,main="SD Intervals"))
}
# Kmeans (only residuals)
groups_kmeans <- get_groups_kmeans(df, residuals,resilience_sign,outcome_string = outcome_string,adversity_string = adversity_string)
df_n_groups <- rbind(df_n_groups,
data.frame(resilient = sum(groups_kmeans=="resilient", na.rm=TRUE), average = sum(groups_kmeans=="average", na.rm=TRUE), vulnerable = sum(groups_kmeans=="vulnerable", na.rm=TRUE), row.names=c("Kmeans")))
df_result[["Kmeans"]] <- groups_kmeans
if(visualization){
print(visualization_groups(df,adversity_string,outcome_string,lm_adjusted,groups_kmeans,main="Groups using k-means algorithm"))
}
return(list(df_result=df_result,df_n_groups=df_n_groups))
}
## Functions : classification ####
classification_metrics <- function(true_labels, predicted_labels) {
# Convert to factors with same levels
levels <- c("resilient", "average", "vulnerable")
true_labels <- factor(true_labels, levels = levels)
predicted_labels <- factor(predicted_labels, levels = levels)
# Confusion Matrix
cm <- caret::confusionMatrix(predicted_labels, true_labels)
# Extract raw table
cm_table <- cm$table
n_classes <- length(levels)
# Print
print(cm_table)
# Initialize results list
results <- list()
# Overall accuracy
results$accuracy <- cm$overall["Accuracy"]
# Balanced accuracy = mean(recall per class)
recalls <- numeric(n_classes)
precisions <- numeric(n_classes)
f1s <- numeric(n_classes)
fns <- numeric(n_classes)
fps <- numeric(n_classes)
supports <- numeric(n_classes)
for (i in 1:n_classes) {
class <- levels[i]
TP <- cm_table[i, i]
FN <- sum(cm_table[, i]) - TP
FP <- sum(cm_table[i, ]) - TP
TN <- sum(cm_table) - TP - FP - FN
recall <- if ((TP + FN) == 0) NA else TP / (TP + FN)
precision <- if ((TP + FP) == 0) NA else TP / (TP + FP)
f1 <- if (is.na(precision) || is.na(recall) || (precision + recall) == 0) NA else 2 * (precision * recall) / (precision + recall)
recalls[i] <- recall
precisions[i] <- precision
f1s[i] <- f1
fns[i] <- FN
fps[i] <- FP
supports[i] <- sum(cm_table[, i])
}
names(recalls) <- levels
names(precisions) <- levels
names(f1s) <- levels
names(fns) <- paste0("FN_", levels)
names(fps) <- paste0("FP_", levels)
names(supports) <- levels
results$macro_recall <- mean(recalls, na.rm = TRUE)
results$macro_precision <- mean(precisions,na.rm=TRUE)
results$f1_per_class <- f1s
results$precision_per_class <- precisions
results$recall_per_class <- recalls
results$macro_f1 <- mean(f1s, na.rm = TRUE)
results$false_negatives <- fns
results$false_positives <- fps
results$support <- supports
return(results)
}
estimation_classification <- function(df,df_result,item_name,list_group_names,method="classification_tree",predictors=c("T1_Sex", "T1_Age", paste0("T1_CYRM_", 1:28))){
set.seed(1) # For reproductibility
res <- data.frame()
for(i in 1:length(list_group_names)){
# Get the grouping
group_name <- list_group_names[[i]]
print(group_name)
# We get the groups for the chosen grouping for all 3 items
df[["groups"]] <- df_result[[group_name]]
y <- df[["groups"]]
X <- df[, predictors]
# We choose the classification method and look at the results
if(method=="classification_tree"){
formula <- as.formula(paste("groups ~", paste(predictors, collapse = " + ")))
arbre <- rpart(formula, data = df, method = "class")
predictions <- predict(arbre, type = "class")
}
else if(method=="LDA"){
model_lda <- lda(X,y)
predictions <- predict(model_lda, X)$class
}
else if(method=="Naive_Bayes"){
formula <- as.formula(paste("groups ~", paste(predictors, collapse = " + ")))
model_nb <- naiveBayes(formula, data = df)
predictions <- predict(model_nb,newdata=df,type="class")
}
else if(method=="Logistic_Regression"){
df[["groups"]] <- factor(df[["groups"]], levels = c("resilient", "average", "vulnerable"))
formula <- as.formula(paste("groups ~", paste(predictors, collapse = " + ")))
model_fit <- multinom_reg() |> fit(formula, data = df)
preds <- model_fit |> augment(new_data = df)
predictions <- preds$.pred_class
}
# Metrics
metrics <- classification_metrics(df[["groups"]],predictions)
res <- rbind(res, data.frame(
group_name = group_name,
accuracy = metrics$accuracy[[1]],
null_model = metrics$support[["average"]] / (metrics$support[["resilient"]]+metrics$support[["average"]]+metrics$support[["vulnerable"]]),
macro_precision = metrics$macro_precision[[1]],
macro_recall = metrics$macro_recall[[1]],
macro_f1 = metrics$macro_f1[[1]],
precision_resilient = metrics$precision_per_class[["resilient"]],
recall_resilient = metrics$recall_per_class[["resilient"]],
f1score_resilient = metrics$f1_per_class[["resilient"]],
precision_average = metrics$precision_per_class[["average"]],
recall_average = metrics$recall_per_class[["average"]],
f1score_average = metrics$f1_per_class[["average"]],
precision_vulnerable = metrics$precision_per_class[["vulnerable"]],
recall_vulnerable = metrics$recall_per_class[["vulnerable"]],
f1score_vulnerable = metrics$f1_per_class[["vulnerable"]],
support_resilient = metrics$support[["resilient"]],
support_average = metrics$support[["average"]],
support_vulnerable = metrics$support[["vulnerable"]]
))
}
return(res)
}
estimation_classification_tree_with_test <- function(df, df_result, item_name, list_group_names, n_perm = 100, predictors=c("T1_Sex", "T1_Age", paste0("T1_CYRM_", 1:28))) {
set.seed(1) # For reproducibility
res <- data.frame()
for (i in seq_along(list_group_names)) {
group_name <- list_group_names[[i]]
cat("Processing:", group_name, "\n")
df[["groups"]] <- df_result[[group_name]]
y <- df[["groups"]]
X <- df[, predictors]
# Build and evaluate model on real data
formula <- as.formula(paste("groups ~", paste(predictors, collapse = " + ")))
arbre <- rpart(formula, data = df, method = "class")
predictions <- predict(arbre, type = "class")
metrics <- classification_metrics(df[["groups"]], predictions)
acc_obs <- metrics$accuracy[[1]]
# Permutation test
perm_accuracies <- numeric(n_perm)
for (j in 1:n_perm) {
df$groups_perm <- sample(df[["groups"]])  # shuffle labels
arbre_perm <- rpart(groups_perm ~ ., data = cbind(df[predictors], groups_perm = df$groups_perm), method = "class")
preds_perm <- predict(arbre_perm, type = "class")
perm_accuracies[j] <- mean(preds_perm == df$groups_perm)
}
p_value <- mean(perm_accuracies >= acc_obs)
# Add the result to the global dataframe
res <- rbind(res, data.frame(
group_name = group_name,
accuracy = acc_obs,
null_model = metrics$support[["average"]] / sum(unlist(metrics$support)),
p_value_permutation = p_value,
macro_precision = metrics$macro_precision[[1]],
macro_recall = metrics$macro_recall[[1]],
macro_f1 = metrics$macro_f1[[1]],
precision_resilient = metrics$precision_per_class[["resilient"]],
recall_resilient = metrics$recall_per_class[["resilient"]],
f1score_resilient = metrics$f1_per_class[["resilient"]],
precision_average = metrics$precision_per_class[["average"]],
recall_average = metrics$recall_per_class[["average"]],
f1score_average = metrics$f1_per_class[["average"]],
precision_vulnerable = metrics$precision_per_class[["vulnerable"]],
recall_vulnerable = metrics$recall_per_class[["vulnerable"]],
f1score_vulnerable = metrics$f1_per_class[["vulnerable"]],
support_resilient = metrics$support[["resilient"]],
support_average = metrics$support[["average"]],
support_vulnerable = metrics$support[["vulnerable"]]
))
}
return(res)
}
## PART II : Engagement depending on BDI-II ####
dim(df_SA)
# Variables used
residuals_vars <- c("T1_SES_total_SA","T1_WES_total", "T1_BDI_II","T1_edu_1a")
explication_vars <- c("T1_Sex","T1_Age",paste0("T1_CYRM_", 1:28),paste0("T1_PoNS_",1:8),paste0("T1_SF15_",1:15),paste0("T1_CPTS_",1:20),paste0("T1_FAS_",1:9),paste0("T1_BCE_",1:10))
#explication_vars_sum <- c("T1_Sex","T1_Age","T1_CYRM28_total","T1_PoNS","T1_SF_14_PHC","T1_CPTS","T1_FAS","T1_BCE")
# Dataframe with SES or WES + pertinent variables
df_SAr <- df_SA[(!is.na(df_SA$T1_SES_total_SA)|!is.na(df_SA$T1_WES_total)) & !is.na(df_SA$T1_BDI_II)&!is.na(df_SA$T1_CYRM_10),c(residuals_vars,explication_vars)]
dim(df_SAr) # 423 individuals
# Impute data
df_SAr_wtWESSES <- df_SAr[explication_vars]
df_SAr_wtWESSES <- as.data.frame(lapply(df_SAr_wtWESSES, function(x) as.numeric(as.character(x))))
df_SAr_wtWESSES.mf <- missForest::missForest(xmis = df_SAr_wtWESSES)
df_SAr[,explication_vars] <- df_SAr_wtWESSES.mf$ximp
#visdat::vis_miss(df_SAr)
# Creation of the engagement variable
n <- nrow(df_SAr)
for(i in 1:n){
if(is.na(df_SAr[i,"T1_WES_total"])){
df_SAr[i,"T1_Engagement"] <- (df_SAr[i,"T1_SES_total_SA"]-33)/(165-33)*100
}
else if(is.na(df_SAr[i,"T1_SES_total_SA"])){
df_SAr[i,"T1_Engagement"] <- (df_SAr[i,"T1_WES_total"]-9)/(63-9)*100
}
else{# If both are not NA
if(df_SAr[i,"T1_edu_1a"] %in% 9:12){
df_SAr[i,"T1_Engagement"] <- (df_SAr[i,"T1_SES_total_SA"]-33)/(165-33)*100
}
else{
df_SAr[i,"T1_Engagement"] <- (df_SAr[i,"T1_SES_total_SA"]-33)/(165-33)*100
}
}
}
View(df_SAr)
# Groupings for each method : df_result with groups and df_n_groups with sizes
df <- df_SAr
adversity_string <- "T1_BDI_II"
outcome_string <- "T1_Engagement"
outcome <- df$T1_Engagement
bins <- bins_BDI_II
res <- adjusted_fit(df_SAr,adversity="T1_BDI_II",outcome="T1_Engagement",main="Adjusted and unadjusted linear regression of Engagement (work/school) with BDI-II score",xlab="BDI-II score",ylab="Engagement")
lm_adjusted <- res$lm_adjusted
lm_adjusted_cred <- res$lm_adjusted_cred
residuals <- res$residuals_adjusted
all_groups_BDI_Engagement <- get_all_groups(df,adversity_string,outcome_string,bins,res,modification="nothing",visualization = TRUE)
df_result_BDI_Engagement <- all_groups_BDI_Engagement$df_result
df_n_groups_BDI_Engagement <- all_groups_BDI_Engagement$df_n_groups
View(df_result_BDI_Engagement)
View(df_n_groups_BDI_Engagement)
# Classification
groups_to_test <- list("quantiles (5%)",
"quantiles (10%)",
"quantiles (15%)",
"quantiles (20%)",
"quantiles (25%)",
"pred. residuals (75%)",
"pred. residuals (60%)",
"pred. residuals (50%)",
"conf. residuals (99%)",
"conf. residuals (95%)",
"cred. 99.9%",
"cred. 99%",
"cred. 95%",
"cred. 90%",
"cred. 75%",
"cred. 50%",
"2SD",
"1SD",
"0.5SD",
"Kmeans")
groups_to_test_small <- list("quantiles (5%)",
"quantiles (10%)",
"quantiles (15%)",
"quantiles (20%)",
"quantiles (25%)",
"pred. residuals (75%)",
"pred. residuals (60%)",
"pred. residuals (50%)",
"2SD",
"1SD",
"0.5SD",
"Kmeans")
df_perf_classification_tree <- estimation_classification_tree_with_test(df,df_result_BDI_Engagement,"Engagement",groups_to_test,n_perm=100,predictors = explication_vars)
# PCA/EFA on Perception of Neighborhood
res.PCA<-PCA(df_SAr[, c(paste0("T1_PoNS_", 1:8))],graph=FALSE)
res.PCA$eig
fviz_screeplot(X=res.PCA, addlabels = TRUE, ylim = c(0, 50))
fviz_pca_var(res.PCA, axes = 1 :2,col.var = "cos2",gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE)
fviz_pca_var(res.PCA, axes = 3:4,col.var = "cos2",gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE)
corrplot(cor(df_SAr[, c(paste0("T1_PoNS_", 1:8))]))
#Comments : We can see two groups emerge for the variable :
# 1,2,4,5,7 which are positively correlated together and are characterized by relatively big value on dim1 and small positive value on dim2. We don't see the group again on dim 3 and 4.
# 3, 6 and 8 which are positively correlated together and are characterized by a small negative value on dim1 and a relatively big value on dim2. Again we don't see this grouping again.
# Good to note that questions 3,6 and 8 are the questions that need to be "reversed".
# The two groups are at a 90Â° angle which would mean a small correlation between them.
# LPA for engagement and BDI
df_SAr[,c("T1_BDI_II","T1_Engagement")] %>%
dplyr::select(T1_BDI_II, T1_Engagement) %>%
single_imputation() %>%
estimate_profiles(2:6,
variances = c("equal","varying","equal"),
covariances = c("zero","zero","equal"),nrep = 5) %>%
compare_solutions(statistics = c("AIC", "BIC"))
# Best model according to AIC is Model 3 with 5 classes.
df_SAr[,c("T1_BDI_II","T1_Engagement")] %>%
dplyr::select(T1_BDI_II, T1_Engagement) %>%
single_imputation() %>%
estimate_profiles(5,variances=c("equal"),covariances=c("equal")) %>%
plot_profiles()
# Best model according to BIC is Model 1 with 3 classes.
df_SAr[,c("T1_BDI_II","T1_Engagement")] %>%
dplyr::select(T1_BDI_II, T1_Engagement) %>%
single_imputation() %>%
estimate_profiles(3,variances=c("equal"),covariances=c("zero")) %>%
plot_profiles()
# Best model according to analytic hierarchy process is Model 2 with 3 classes.
df_SAr[,c("T1_BDI_II","T1_Engagement")] %>%
dplyr::select(T1_BDI_II, T1_Engagement) %>%
single_imputation() %>%
estimate_profiles(3,variances=c("varying"),covariances=c("zero")) %>%
plot_profiles()
# Regression tree to predict transformed residuals
# Regression tree to predict residuals
